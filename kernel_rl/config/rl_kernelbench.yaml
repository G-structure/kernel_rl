# KernelBench RL Training Configuration
# =====================================
#
# Default configuration: Kevin mode (multi-turn) + RA-ICL
# This combines retrieval-augmented prompts with multi-turn refinement.
#
# Usage:
#   python -m kernel_rl.scripts.train_kernel_rl \
#       --config kernel_rl/config/rl_kernelbench.yaml \
#       log_path=./runs/my_experiment

# Model Configuration
# -------------------
model_name: "Qwen/Qwen3-30B-A3B"
lora_rank: 32
learning_rate: 0.0001

# Generation Configuration
# ------------------------
max_tokens: 4096
temperature: 1.0

# Training Mode
# -------------
mode: "multi_turn"  # Kevin-style multi-turn refinement (default)

# Multi-Turn Configuration (Kevin Mode)
# --------------------------------------
max_turns: 4        # Maximum refinement attempts per problem
gamma: 0.4          # Discount factor for future rewards

# Training Configuration
# ----------------------
num_substeps: 1
loss_fn: "importance_sampling"

# KL Regularization (optional)
kl_penalty_coef: 0.0
kl_discount_factor: 0.0

# Remove groups with constant rewards (no learning signal)
remove_constant_reward_groups: true

# Logging and Checkpointing
# -------------------------
log_path: "./runs/kernel_rl"
save_every: 10
eval_every: 10

# Weights & Biases (optional)
wandb_project: null
wandb_name: null

# Dataset Configuration
# ---------------------
dataset_builder:
  # Problem selection
  level: 1
  start_problem: null  # null = start from beginning
  end_problem: null    # null = use all problems
  backend: "triton"
  dataset_src: "huggingface"

  # Training configuration
  # Note: For multi-turn, effective samples = batch_size * group_size * max_turns
  batch_size: 2      # Reduced for multi-turn overhead
  group_size: 4      # Rollouts per problem
  num_epochs: 1
  shuffle: true

  # Evaluation configuration
  num_correct_trials: 5
  measure_performance: false  # Set true for speed rewards

  # Reward configuration (per-step scores, converted to discounted returns)
  reward_format_weight: 0.1
  reward_compile_weight: 0.2
  reward_correctness_weight: 1.0
  reward_speed_weight: 0.0  # Enable for stage 3 training
  reward_length_weight: 0.05  # Tie-breaking for uniform rewards (GRPO-LEAD style)

  # Test split
  test_fraction: 0.1

  # Renderer (should match model)
  renderer_name: "qwen3"

  # RA-ICL Configuration (retrieval-augmented prompts)
  prompt_option: "raicl"
  rag_index_path: "./kernel_rag_index"
  raicl_k: 3  # Number of examples to retrieve
